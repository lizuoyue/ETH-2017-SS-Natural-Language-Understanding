model: BasicSeq2Seq
model_params:
  # attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  # attention.params:
    # num_units: 512
  bridge.class: seq2seq.models.bridges.InitialStateBridge
  embedding.dim: 128
  # inference.beam_search.beam_width: 8
  encoder.class: seq2seq.encoders.UnidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params:
        num_units: 256
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  decoder.class: seq2seq.decoders.BasicDecoder
  decoder.params:
    max_decode_length: 50
    rnn_cell:
      cell_class: LSTMCell
      cell_params:
        num_units: 256
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      # num_layers: 4
      num_layers: 2
  optimizer.name: SGD
  # optimizer.params:
  #   epsilon: 0.0000008
  # optimizer.learning_rate: 0.0001
  optimizer.learning_rate: 0.2
  optimizer.lr_decay_type: exponential_decay
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_rate: 0.94
  optimizer.lr_staircase: True
  optimizer.clip_gradients: 5.0
  source.max_seq_len: 50
  source.reverse: True
  target.max_seq_len: 50
